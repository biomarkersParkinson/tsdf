{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "These are some examples on how to read and write TSDF data into and from a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload modules automatically on changes; useful for developing\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tsdf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process an existing binary file and write the new TSDF metadata\n",
    "Read and process an existing binary data (accompanied by the TSDF metadata), process the data and save it in the new format, with the corresponding TSDF metadata file.\n",
    "\n",
    "### Load dummy data and see its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type used for storing:\t int16\n",
      "Data dimensions:\t\t (10, 3)\n",
      "Number of rows:\t\t\t 10\n"
     ]
    }
   ],
   "source": [
    "TESTDATA_DIR = os.path.join(\"..\",\"tests\", \"data\")\n",
    "dummy_data_name = \"dummy_10_3_int16\"\n",
    "\n",
    "dummy_metadata = tsdf.load_metadata_from_path(os.path.join(TESTDATA_DIR, dummy_data_name + \".json\"))[dummy_data_name + \".bin\"]\n",
    "dummy_data = dummy_metadata.load_binary()\n",
    "print(f\"Data type used for storing:\\t {dummy_data.dtype}\")\n",
    "print(f\"Data dimensions:\\t\\t {dummy_data.shape}\")\n",
    "print(f\"Number of rows:\\t\\t\\t {dummy_data.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform light data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type used for storing:\t float32\n",
      "Data dimensions:\t\t (10, 3)\n",
      "Number of rows:\t\t\t 10\n"
     ]
    }
   ],
   "source": [
    "processed_dummy_data_1 = (dummy_data / 10).astype('float32')\n",
    "print(f\"Data type used for storing:\\t {processed_dummy_data_1.dtype}\")\n",
    "print(f\"Data dimensions:\\t\\t {processed_dummy_data_1.shape}\")\n",
    "print(f\"Number of rows:\\t\\t\\t {processed_dummy_data_1.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata available from the data (NumPy array)\n",
    "The metadata will be used indirectly to generate the new TSDF metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary formatting that can be inferred from the NumPy array:\n",
      "{'data_type': 'float', 'bits': 32, 'endianness': 'little', 'rows': 10}\n"
     ]
    }
   ],
   "source": [
    "bin_meta = tsdf.get_metadata_from_ndarray(processed_dummy_data_1)\n",
    "print(f\"Binary formatting that can be inferred from the NumPy array:\\n{bin_meta}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the processed data \n",
    "Write the processed data in binary format. The call returns the corresponding metadata object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dummy_data_name_1 = \"tmp_test_dummy_10_3_int16_to_float32\"\n",
    "processed_dummy_metadata_1 = tsdf.write_binary_file(\n",
    "            TESTDATA_DIR,\n",
    "            processed_dummy_data_name_1 + \".bin\",\n",
    "            processed_dummy_data_1,\n",
    "            dummy_metadata.get_plain_tsdf_dict_copy(),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the TSDF metadata file that describes the processed binary data format\n",
    "\n",
    "#### 1) Write the metadata file for a single binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new metadata file\n",
    "tsdf.write_metadata([processed_dummy_metadata_1], processed_dummy_data_name_1 + \".json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Write a metadata file that combines multiple binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_test_dummy_10_3_int16_to_float32.bin\n",
      "tmp_test_dummy_10_3_int16_to_float32.bin\n",
      "tmp_test_dummy_10_3_int16_to_float32.bin\n",
      "tmp_test_dummy_10_3_int16_to_int32.bin\n",
      "tmp_test_dummy_10_3_int16_to_int32.bin\n",
      "tmp_test_dummy_10_3_int16_to_int32.bin\n",
      "[0.00469378, 0.00469378, 0.00469378]\n",
      "[0.00469378, 0.00469378, 0.00469378]\n",
      "[0.00469378, 0.00469378, 0.00469378]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'scale_factors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m processed_dummy_metadata_2 \u001b[39m=\u001b[39m tsdf\u001b[39m.\u001b[39mwrite_binary_file(\n\u001b[1;32m     12\u001b[0m             TESTDATA_DIR,\n\u001b[1;32m     13\u001b[0m             processed_dummy_data_name_2 \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.bin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m             processed_dummy_data_2,\n\u001b[1;32m     15\u001b[0m             updated_dummy_metadata,\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     18\u001b[0m \u001b[39m# Write a metadata file that combines the two binary files\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m tsdf\u001b[39m.\u001b[39;49mwrite_metadata([processed_dummy_metadata_1, processed_dummy_metadata_2], \u001b[39m\"\u001b[39;49m\u001b[39mtmp_test_dummy_10_3_int16_to_int_n_float.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/git/biomarkers_repo/tsdf/src/tsdf/io.py:148\u001b[0m, in \u001b[0;36mwrite_metadata\u001b[0;34m(metadatas, file_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overlap:\n\u001b[1;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m TSDFMetadataFieldValueError(\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMetadata files mist have at least one common field. Otherwise, they should be stored separately.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     )\n\u001b[0;32m--> 148\u001b[0m overlap[\u001b[39m\"\u001b[39m\u001b[39msensors\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m calculate_ovelaps_rec(plain_meta)\n\u001b[1;32m    149\u001b[0m write_to_file(overlap, metadatas[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfile_dir_path, file_name)\n",
      "File \u001b[0;32m~/git/biomarkers_repo/tsdf/src/tsdf/io.py:180\u001b[0m, in \u001b[0;36mcalculate_ovelaps_rec\u001b[0;34m(metadatas)\u001b[0m\n\u001b[1;32m    177\u001b[0m final_metadata: List[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m [] \u001b[39m# The metadata that is left to be processed\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m get_all_keys(metadatas):\n\u001b[0;32m--> 180\u001b[0m     overlap_per_key[key] \u001b[39m=\u001b[39m calculate_max_overlap(metadatas, key)\n\u001b[1;32m    182\u001b[0m max_key \u001b[39m=\u001b[39m max_len_key(overlap_per_key)\n\u001b[1;32m    184\u001b[0m first_group \u001b[39m=\u001b[39m overlap_per_key[max_key]\n",
      "File \u001b[0;32m~/git/biomarkers_repo/tsdf/src/tsdf/io.py:218\u001b[0m, in \u001b[0;36mcalculate_max_overlap\u001b[0;34m(meta_files, meta_key)\u001b[0m\n\u001b[1;32m    216\u001b[0m values : Dict[\u001b[39mstr\u001b[39m, List[Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m {} \u001b[39m# Key: a value for the given meta_key, Value: list of metadata files that have that value\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m meta \u001b[39min\u001b[39;00m meta_files:\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mprint\u001b[39m(meta[meta_key])\n\u001b[1;32m    219\u001b[0m     curr_value \u001b[39m=\u001b[39m meta[meta_key]\n\u001b[1;32m    220\u001b[0m     \u001b[39mprint\u001b[39m(curr_value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'scale_factors'"
     ]
    }
   ],
   "source": [
    "# Preprocess the original data to generate another data source\n",
    "processed_dummy_data_2 = (dummy_data * 1000).astype('int32')\n",
    "\n",
    "# Adjust the metadata slightly\n",
    "updated_dummy_metadata = dummy_metadata.get_plain_tsdf_dict_copy()\n",
    "updated_dummy_metadata.pop(\"scale_factors\") #remove the 'scale_factors'\n",
    "\n",
    "\n",
    "# Save the new binary file\n",
    "processed_dummy_data_name_2 = \"tmp_test_dummy_10_3_int16_to_int32\"\n",
    "processed_dummy_metadata_2 = tsdf.write_binary_file(\n",
    "            TESTDATA_DIR,\n",
    "            processed_dummy_data_name_2 + \".bin\",\n",
    "            processed_dummy_data_2,\n",
    "            updated_dummy_metadata,\n",
    "        )\n",
    "\n",
    "# Write a metadata file that combines the two binary files\n",
    "tsdf.write_metadata([processed_dummy_metadata_1, processed_dummy_metadata_2], \"tmp_test_dummy_10_3_int16_to_int_n_float.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genarate a new binary file and the corresponding TSDF metadata\n",
    "Generate binary data and save it and the corresponding TSDF metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(TESTDATA_DIR, \u001b[39m\"\u001b[39m\u001b[39mtest_output_1.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m rs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m data_1 \u001b[39m=\u001b[39m rs\u001b[39m.\u001b[39mrand(\u001b[39m17\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "path = os.path.join(TESTDATA_DIR, \"test_output_1.bin\")\n",
    "rs = np.random.RandomState(seed=42)\n",
    "data_1 = rs.rand(17, 1).astype(np.float32)\n",
    "data_2 = rs.rand(15, 2).astype(np.int16)\n",
    "\n",
    "\n",
    "# An example where the metadata is defined from scratch\n",
    "new_metadata = {}\n",
    "new_metadata[\"subject_id\"] = \"example\"\n",
    "new_metadata[\"study_id\"] = \"example\"\n",
    "new_metadata[\"device_id\"] = \"example\"\n",
    "new_metadata[\"endianness\"] = \"little\"\n",
    "new_metadata[\"metadata_version\"] = \"0.1\"\n",
    "new_metadata[\"start_datetime_unix_ms\"] = 1571135957025,\n",
    "new_metadata[\"start_iso8601\"] = \"2019-10-15T10:39:17.025000+00:00\"\n",
    "new_metadata[\"end_datetime_unix_ms\"] = 1571168851826\n",
    "new_metadata[\"end_iso8601\"] = \"2019-10-15T19:47:31.826000+00:00\"\n",
    "new_metadata[\"channels\"] = [\"x\",\"y\",\"z\"]\n",
    "new_metadata[\"units\"] = [\"m/s/s\",\"m/s/s\",\"m/s/s\"]\n",
    "\n",
    "# Write the two binary files based on the provided metadata\n",
    "\n",
    "file_prefix = \"tmp_test\"\n",
    "new_meta_1 =  tsdf.write_binary_file(TESTDATA_DIR, file_prefix + \"_1.bin\", data_1, new_metadata)\n",
    "new_meta_2 =  tsdf.write_binary_file(TESTDATA_DIR, file_prefix+\"_2.bin\", data_2, new_metadata)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metadata that corresponds to the binary data. In case of multiple binary files, the corresponding metadata files have to be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the first metadata file\n",
    "tsdf.write_metadata([new_meta_1], file_prefix +\"_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and write both metadata files\n",
    "tsdf.write_metadata([new_meta_1, new_meta_2], file_prefix+\"_2.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsdf-zVA6tG---py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1e978847a77d4ff49203fd09f0f7925f58560bf1007438482d75cb657018d9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
